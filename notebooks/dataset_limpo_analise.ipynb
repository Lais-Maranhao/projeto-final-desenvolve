{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza de Dados para Análise Exploratória\n",
    "\n",
    "Este notebook realiza uma limpeza básica no dataset de treino. O objetivo é preparar os dados para **análise exploratória (EDA)**, e não para treinar um modelo de machine learning. \n",
    "\n",
    "As seguintes etapas são realizadas:\n",
    "1. Remoção de linhas duplicadas.\n",
    "2. Tratamento de valores ausentes (NaN).\n",
    "3. Remoção de outliers em colunas numéricas.\n",
    "4. Agrupamento de categorias raras em colunas categóricas.\n",
    "\n",
    "O dataset resultante será salvo em `data/train_data_analysis_v2.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importação das Bibliotecas\n",
    "\n",
    "Primeiro, importamos as bibliotecas necessárias. `pandas` é essencial para manipulação de dados e `numpy` para operações numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados\n",
    "\n",
    "Carregamos o dataset original a partir do arquivo `train_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpeza dos Dados\n",
    "\n",
    "Agora, aplicamos as técnicas de limpeza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Remoção de Duplicatas\n",
    "\n",
    "Linhas duplicadas podem distorcer a análise, então garantimos que cada registro seja único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo = df.drop_duplicates()\n",
    "print(f'Linhas removidas por duplicidade: {len(df) - len(df_limpo)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Tratamento de Valores Ausentes\n",
    "\n",
    "Verificamos se há valores ausentes (NaN) e os tratamos. Para a análise, preenchemos os valores ausentes de `Bed Grade` com a moda (o valor mais frequente) e os de `City_Code_Patient` com 0, assumindo que `0` pode representar 'Não informado'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo['Bed Grade'] = df_limpo['Bed Grade'].fillna(df_limpo['Bed Grade'].mode()[0])\n",
    "df_limpo['City_Code_Patient'] = df_limpo['City_Code_Patient'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Remoção de Outliers (Método IQR)\n",
    "\n",
    "Outliers são valores extremos que podem distorcer as métricas estatísticas. Usamos o método do Intervalo Interquartil (IQR) para identificar e remover outliers das colunas numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df_limpo.select_dtypes(include=np.number).columns\n",
    "linhas_antes = len(df_limpo)\n",
    "\n",
    "for col in num_cols:\n",
    "    Q1 = df_limpo[col].quantile(0.25)\n",
    "    Q3 = df_limpo[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lim_inf = Q1 - 1.5 * IQR\n",
    "    lim_sup = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df_limpo = df_limpo[(df_limpo[col] >= lim_inf) & (df_limpo[col] <= lim_sup)]\n",
    "\n",
    "print(f'Linhas removidas por outliers: {linhas_antes - len(df_limpo)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Agrupamento de Categorias Raras\n",
    "\n",
    "Categorias com pouquíssimas ocorrências podem não ser estatisticamente significantes para análise. Agrupamos todas as categorias que representam menos de 1% do total em uma nova categoria chamada 'OUTRA'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df_limpo.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    freq = df_limpo[col].value_counts(normalize=True)\n",
    "    raras = freq[freq < 0.01].index\n",
    "    \n",
    "    if len(raras) > 0:\n",
    "        df_limpo[col] = df_limpo[col].replace(raras, 'OUTRA')\n",
    "        print(f'Categorias raras agrupadas na coluna: {col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Salvando o Dataset Limpo\n",
    "\n",
    "Finalmente, salvamos o dataframe limpo em um novo arquivo CSV para ser usado na análise exploratória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo.to_csv('../data/train_data_analysis_v2.csv', index=False)\n",
    "print('\\nDataset salvo com sucesso em ../data/train_data_analysis_v2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
