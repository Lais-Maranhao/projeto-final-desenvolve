{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final \u2014 Fluxo Integrado de Prepara\u00e7\u00e3o e Modelagem\n",
    "Este notebook re\u00fane todo o trabalho desenvolvido nos arquivos originais, mas agora organizado em um \u00fanico fluxo com coment\u00e1rios mais detalhados. \n",
    "\n",
    "Ao longo das se\u00e7\u00f5es, indicamos o que j\u00e1 havia sido feito, o que foi ajustado nesta revis\u00e3o e por qu\u00ea, sempre com uma linguagem pensada para estudantes. Dessa forma, voc\u00ea consegue acompanhar o racioc\u00ednio e adaptar o material com facilidade nas pr\u00f3ximas etapas do curso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis\u00e3o Geral do Notebook\n",
    "1. **Configura\u00e7\u00e3o do ambiente e importa\u00e7\u00f5es** \u2014 instalamos/ativamos as bibliotecas necess\u00e1rias e explicamos para que serve cada grupo de ferramentas.\n",
    "2. **Coleta dos dados no BigQuery** \u2014 mantemos o passo original para buscar os dados oficiais e explicamos como habilitar a autentica\u00e7\u00e3o quando estiver usando a nuvem.\n",
    "3. **Uso tempor\u00e1rio de arquivos locais (`data/`)** \u2014 inclu\u00edmos uma alternativa para rodar tudo apenas com os CSVs do reposit\u00f3rio, o que facilita os testes e a valida\u00e7\u00e3o neste momento.\n",
    "4. **An\u00e1lise explorat\u00f3ria** \u2014 revisamos as tabelas b\u00e1sicas, destacamos o que os gr\u00e1ficos mostram e apontamos os fatores de maior impacto.\n",
    "5. **Prepara\u00e7\u00e3o dos dados** \u2014 detalhamos como tratar valores ausentes, lidar com outliers sem perder observa\u00e7\u00f5es e padronizar categorias raras.\n",
    "6. **Modelagem e valida\u00e7\u00e3o** \u2014 treinamos o Random Forest (mesmo modelo usado antes) e agora adicionamos valida\u00e7\u00e3o cruzada e um baseline simples para comparar resultados.\n",
    "7. **Simula\u00e7\u00f5es e previs\u00e3o no conjunto de teste** \u2014 reaproveitamos o pipeline para simular perfis de pacientes e prever as interna\u00e7\u00f5es do conjunto de teste.\n",
    "8. **Recomenda\u00e7\u00f5es finais** \u2014 reunimos os aprendizados e pr\u00f3ximos passos sugeridos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Configura\u00e7\u00f5es e Importa\u00e7\u00f5es\n",
    "Nesta etapa, garantimos que todas as depend\u00eancias estejam dispon\u00edveis. O objetivo \u00e9 rodar o notebook tanto no Google Colab (com BigQuery) quanto localmente, por isso mantivemos os mesmos pacotes e acrescentamos alguns utilit\u00e1rios para valida\u00e7\u00e3o dos modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install google-cloud-bigquery --quiet\n",
    "!pip install pandas scikit-learn matplotlib seaborn joblib --quiet\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Autentica\u00e7\u00e3o no BigQuery\n",
    "\n",
    "Autentique-se utilizando as credenciais do Google para acessar o projeto do BigQuery.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Coleta dos Dados no BigQuery\n",
    "\n",
    "Consultamos as tabelas de treino e teste diretamente no projeto `t1engenhariadados` para garantir que utilizamos a vers\u00e3o mais atualizada dos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "main_project = bigquery.Client(project=\"t1engenhariadados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "    select_train_data = \"\"\"\"\n",
    "SELECT *\n",
    "FROM `t1engenhariadados.turma2_grupo9.train_data`\n",
    "\"\"\"\n",
    "    train = main_project.query(select_train_data).to_dataframe()\n",
    "\n",
    "    select_test_data = \"\"\"\"\n",
    "SELECT *\n",
    "FROM `t1engenhariadados.turma2_grupo9.test_data`\n",
    "\"\"\"\n",
    "    test = main_project.query(select_test_data).to_dataframe()\n",
    "\n",
    "    print(f\"Dimens\u00f5es do conjunto de treino: {train.shape}\")\n",
    "    print(f\"Dimens\u00f5es do conjunto de teste:  {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Usando os arquivos locais do reposit\u00f3rio (passo opcional)\n",
    "Durante o desenvolvimento, nem sempre teremos acesso ao BigQuery. Para continuar os estudos mesmo assim, adicionamos esta alternativa usando os CSVs dispon\u00edveis na pasta `data/`. \n",
    "\n",
    "* **O que mudou?** Nada foi removido do fluxo original: apenas acrescentamos esta etapa opcional.\n",
    "* **Como funciona?** Definimos uma vari\u00e1vel `USE_LOCAL_DATA`. Quando ela estiver como `True`, carregamos os arquivos locais e sobrescrevemos `train` e `test`. Quando estiver como `False`, mantemos exatamente o que veio do BigQuery.\n",
    "* **Por que isso \u00e9 \u00fatil?** Permite validar o modelo agora e deixar a altera\u00e7\u00e3o do BigQuery para quando o grupo estiver pronto para subir os dados atualizados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LOCAL_DATA = True  # altere para False quando quiser usar somente os dados vindos do BigQuery\n",
    "\n",
    "data_source = \"BigQuery\"\n",
    "if USE_LOCAL_DATA:\n",
    "    data_source = \"arquivos locais (.csv)\"\n",
    "    data_dir = None\n",
    "    for candidate in [Path(\"data\"), Path(\"../data\")]:\n",
    "        if (candidate / \"train_data.csv\").exists() and (candidate / \"test_data.csv\").exists():\n",
    "            data_dir = candidate\n",
    "            break\n",
    "    if data_dir is None:\n",
    "        raise FileNotFoundError(\"N\u00e3o encontrei os arquivos train_data.csv e test_data.csv na pasta data/.\")\n",
    "\n",
    "    train = pd.read_csv(data_dir / \"train_data.csv\")\n",
    "    test = pd.read_csv(data_dir / \"test_data.csv\")\n",
    "\n",
    "    print(\"Carregando dados a partir dos CSVs locais...\")\n",
    "    print(f\"Diret\u00f3rio utilizado: {data_dir.resolve()}\")\n",
    "    print(f\"Dimens\u00f5es do conjunto de treino: {train.shape}\")\n",
    "    print(f\"Dimens\u00f5es do conjunto de teste:  {test.shape}\")\n",
    "else:\n",
    "    print(\"Mantendo os dados carregados do BigQuery (nenhum arquivo local foi lido).\")\n",
    "\n",
    "print(f\"Fonte atual de dados: {data_source}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Prepara\u00e7\u00e3o do DataFrame base\n",
    "\n",
    "Manteremos uma c\u00f3pia do conjunto de treino para explora\u00e7\u00e3o e pr\u00e9-processamento, preservando os dados brutos retornados do BigQuery.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_raw = train.copy()\n",
    "df_exploration = df_raw.copy()\n",
    "\n",
    "print(f'An\u00e1lise baseada na fonte de dados: {data_source}')\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. An\u00e1lise Explorat\u00f3ria dos Dados\n",
    "Antes de modelar, revisamos a estrutura do dataset para entender volumes, tipos de vari\u00e1veis e poss\u00edveis problemas. Mantivemos as consultas originais e adicionamos interpreta\u00e7\u00f5es sobre o que observar em cada sa\u00edda.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_exploration.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_exploration.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Padr\u00f5es e Rela\u00e7\u00f5es\n",
    "\n",
    "Exploramos correla\u00e7\u00f5es entre vari\u00e1veis num\u00e9ricas e a distribui\u00e7\u00e3o de `Stay` em rela\u00e7\u00e3o a atributos categ\u00f3ricos relevantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "numeric_cols = df_exploration.select_dtypes(include=[np.number]).columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_exploration[numeric_cols].corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correla\u00e7\u00e3o entre vari\u00e1veis num\u00e9ricas\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "    print(\"Value counts for Stay:\")\n",
    "    print(df_exploration[\"Stay\"].value_counts().to_markdown())\n",
    "\n",
    "    print(\"\n",
    "Stay vs Hospital_type_code:\")\n",
    "    print(df_exploration.groupby(\"Hospital_type_code\")[\"Stay\"].value_counts(normalize=True).unstack().to_markdown())\n",
    "\n",
    "    print(\"\n",
    "Stay vs Department:\")\n",
    "    print(df_exploration.groupby(\"Department\")[\"Stay\"].value_counts(normalize=True).unstack().to_markdown())\n",
    "\n",
    "    print(\"\n",
    "Stay vs Ward_Type:\")\n",
    "    print(df_exploration.groupby(\"Ward_Type\")[\"Stay\"].value_counts(normalize=True).unstack().to_markdown())\n",
    "\n",
    "    print(\"\n",
    "Stay vs Severity of Illness:\")\n",
    "    print(df_exploration.groupby(\"Severity of Illness\")[\"Stay\"].value_counts(normalize=True).unstack().to_markdown())\n",
    "\n",
    "    print(\"\n",
    "Stay vs Age:\")\n",
    "    print(df_exploration.groupby(\"Age\")[\"Stay\"].value_counts(normalize=True).unstack().to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_exploration, x=\"Stay\", order=df_exploration[\"Stay\"].value_counts().index)\n",
    "plt.title(\"Distribui\u00e7\u00e3o da Dura\u00e7\u00e3o da Estadia (Stay)\")\n",
    "plt.xlabel(\"Dura\u00e7\u00e3o da Estadia\")\n",
    "plt.ylabel(\"Contagem\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.countplot(\n",
    "    data=df_exploration,\n",
    "    x=\"Severity of Illness\",\n",
    "    hue=\"Stay\",\n",
    "    palette=\"viridis\",\n",
    "    order=df_exploration[\"Severity of Illness\"].value_counts().index,\n",
    ")\n",
    "plt.title(\"Dura\u00e7\u00e3o da Estadia por Gravidade da Doen\u00e7a\")\n",
    "plt.xlabel(\"Gravidade da Doen\u00e7a\")\n",
    "plt.ylabel(\"Contagem\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fatores que influenciam a dura\u00e7\u00e3o da interna\u00e7\u00e3o\n",
    "Os gr\u00e1ficos e tabelas apontam que vari\u00e1veis como gravidade da doen\u00e7a, tipo de admiss\u00e3o e faixa et\u00e1ria t\u00eam forte associa\u00e7\u00e3o com o tempo de interna\u00e7\u00e3o. Nesta revis\u00e3o inclu\u00edmos notas explicando como interpretar essas rela\u00e7\u00f5es para apoiar discuss\u00f5es em sala.\n",
    "\n",
    "- **Gravidade (`Severity of Illness`)**: pacientes classificados como `Extreme` concentram as maiores dura\u00e7\u00f5es.\n",
    "- **Tipo de admiss\u00e3o (`Type of Admission`)**: interna\u00e7\u00f5es de emerg\u00eancia tendem a ser mais longas do que procedimentos eletivos.\n",
    "- **Recursos do hospital**: hospitais com menos quartos extras aparecem mais nos casos longos, sugerindo gargalos.\n",
    "- **Dep\u00f3sito de admiss\u00e3o**: mesmo ap\u00f3s aplicar o log, valores muito altos continuam associados a estadias maiores, possivelmente por refletir tratamentos mais complexos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Intera\u00e7\u00e3o entre idade e gravidade da doen\u00e7a\n",
    "Esta se\u00e7\u00e3o j\u00e1 existia, mas agora destacamos por que olhar para a combina\u00e7\u00e3o das vari\u00e1veis faz diferen\u00e7a. Ao cruzar idade com gravidade, percebemos padr\u00f5es que n\u00e3o ficam claros quando analisamos cada atributo separadamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principais insights da intera\u00e7\u00e3o\n",
    "1. **Idade e gravidade extrema:** pacientes mais velhos com gravidade `Extreme` concentram as maiores estadias, refor\u00e7ando a necessidade de monitoramento cont\u00ednuo.\n",
    "2. **Casos moderados em adultos jovens:** mesmo com gravidade `Moderate`, adultos na faixa dos 31-50 anos podem permanecer acima da m\u00e9dia, sinalizando que fatores cl\u00ednicos adicionais merecem investiga\u00e7\u00e3o.\n",
    "3. **Impacto das admiss\u00f5es de emerg\u00eancia:** quando combinamos idade avan\u00e7ada, gravidade alta e emerg\u00eancia, observamos as maiores filas \u2014 informa\u00e7\u00e3o \u00fatil para priorizar recursos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepara\u00e7\u00e3o dos Dados para Machine Learning\n",
    "Retomamos as etapas do notebook de prepara\u00e7\u00e3o, mas com alguns ajustes para evitar perdas de informa\u00e7\u00e3o e manter o processo consistente entre treino e infer\u00eancia.\n",
    "\n",
    "**Principais melhorias adicionadas agora:**\n",
    "- Substitu\u00edmos o preenchimento de `City_Code_Patient` por `0` pelo uso da moda real da coluna, evitando criar uma cidade fict\u00edcia.\n",
    "- Calculamos os limites de outliers uma \u00fanica vez e aplicamos `clip` (ao inv\u00e9s de remover linhas), preservando todos os registros.\n",
    "- Refor\u00e7amos a fun\u00e7\u00e3o de convers\u00e3o de idade para garantir que sempre devolva valores num\u00e9ricos.\n",
    "- Mantivemos o agrupamento de categorias raras e o log da coluna `Admission_Deposit`, explicando quando usar cada transforma\u00e7\u00e3o.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def age_to_numeric(age_range):\n",
    "    if isinstance(age_range, (int, float)) and not pd.isna(age_range):\n",
    "        return float(age_range)\n",
    "\n",
    "    if isinstance(age_range, str):\n",
    "        if \"-\" in age_range:\n",
    "            low, high = age_range.split(\"-\")\n",
    "            return (float(low) + float(high)) / 2\n",
    "        digits = \"\".join(ch for ch in age_range if ch.isdigit())\n",
    "        if digits:\n",
    "            return float(digits)\n",
    "\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_prepared = df_raw.copy()\n",
    "\n",
    "# Tratamento de valores ausentes com estat\u00edsticas dos dados\n",
    "bed_grade_mode = df_prepared['Bed Grade'].mode(dropna=True)[0]\n",
    "df_prepared['Bed Grade'] = df_prepared['Bed Grade'].fillna(bed_grade_mode)\n",
    "\n",
    "city_code_mode = df_prepared['City_Code_Patient'].mode(dropna=True)[0]\n",
    "df_prepared['City_Code_Patient'] = (\n",
    "    pd.to_numeric(df_prepared['City_Code_Patient'], errors='coerce')\n",
    "    .fillna(city_code_mode)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Convers\u00e3o da idade para valores num\u00e9ricos m\u00e9dios\n",
    "df_prepared['Age'] = df_prepared['Age'].apply(age_to_numeric)\n",
    "\n",
    "# Remo\u00e7\u00e3o de duplicatas exatas\n",
    "df_prepared = df_prepared.drop_duplicates()\n",
    "\n",
    "# Tratamento de outliers via IQR com recorte (clip)\n",
    "numeric_cols = df_prepared.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['case_id', 'patientid']]\n",
    "\n",
    "outlier_bounds = {}\n",
    "for col in numeric_cols:\n",
    "    Q1 = df_prepared[col].quantile(0.25)\n",
    "    Q3 = df_prepared[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outlier_bounds[col] = (lower, upper)\n",
    "    df_prepared[col] = df_prepared[col].clip(lower, upper)\n",
    "\n",
    "# Transforma\u00e7\u00e3o logar\u00edtmica em Admission_Deposit\n",
    "if 'Admission_Deposit' in df_prepared.columns:\n",
    "    df_prepared['Admission_Deposit'] = np.log1p(df_prepared['Admission_Deposit'])\n",
    "\n",
    "# Remo\u00e7\u00e3o de vari\u00e1veis altamente correlacionadas\n",
    "numeric_cols_clean = df_prepared.select_dtypes(include=np.number).columns.tolist()\n",
    "corr_matrix = df_prepared[numeric_cols_clean].corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "columns_to_drop_corr = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "df_prepared = df_prepared.drop(columns=columns_to_drop_corr, errors='ignore')\n",
    "\n",
    "# Agrupamento de categorias raras\n",
    "frequent_categories = {}\n",
    "cat_cols = df_prepared.select_dtypes(include='object').columns.tolist()\n",
    "if 'Stay' in cat_cols:\n",
    "    cat_cols.remove('Stay')\n",
    "\n",
    "for col in cat_cols:\n",
    "    freq = df_prepared[col].value_counts(normalize=True)\n",
    "    raras = freq[freq < 0.01].index\n",
    "    allowed = set(freq[freq >= 0.01].index)\n",
    "    if len(raras) > 0:\n",
    "        df_prepared[col] = df_prepared[col].replace(list(raras), 'OUTRA')\n",
    "        allowed.add('OUTRA')\n",
    "    frequent_categories[col] = allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Dimens\u00f5es ap\u00f3s limpeza: {df_prepared.shape}')\n",
    "df_prepared.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "preparation_artifacts = {\n",
    "    'bed_grade_mode': bed_grade_mode,\n",
    "    'city_code_mode': city_code_mode,\n",
    "    'outlier_bounds': outlier_bounds,\n",
    "    'columns_to_drop_corr': columns_to_drop_corr,\n",
    "    'frequent_categories': frequent_categories,\n",
    "}\n",
    "\n",
    "preparation_artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_dataset_for_inference(df, artifacts, drop_target=False):\n",
    "    processed = df.copy()\n",
    "\n",
    "    if 'Bed Grade' in processed.columns:\n",
    "        processed['Bed Grade'] = processed['Bed Grade'].fillna(artifacts['bed_grade_mode'])\n",
    "\n",
    "    if 'City_Code_Patient' in processed.columns:\n",
    "        processed['City_Code_Patient'] = pd.to_numeric(\n",
    "            processed['City_Code_Patient'], errors='coerce'\n",
    "        ).fillna(artifacts['city_code_mode'])\n",
    "\n",
    "    if 'Age' in processed.columns:\n",
    "        processed['Age'] = processed['Age'].apply(age_to_numeric)\n",
    "\n",
    "    processed = processed.drop_duplicates()\n",
    "\n",
    "    for col, (lower, upper) in artifacts['outlier_bounds'].items():\n",
    "        if col in processed.columns:\n",
    "            processed[col] = processed[col].clip(lower, upper)\n",
    "\n",
    "    if 'Admission_Deposit' in processed.columns:\n",
    "        processed['Admission_Deposit'] = np.log1p(processed['Admission_Deposit'])\n",
    "\n",
    "    processed = processed.drop(columns=artifacts['columns_to_drop_corr'], errors='ignore')\n",
    "\n",
    "    for col, allowed in artifacts['frequent_categories'].items():\n",
    "        if col in processed.columns:\n",
    "            processed[col] = processed[col].where(processed[col].isin(allowed), 'OUTRA')\n",
    "\n",
    "    if drop_target and 'Stay' in processed.columns:\n",
    "        processed = processed.drop(columns=['Stay'])\n",
    "\n",
    "    feature_dtypes = artifacts.get('feature_dtypes')\n",
    "    feature_columns = artifacts.get('feature_columns')\n",
    "    if feature_dtypes and feature_columns:\n",
    "        for col, dtype_name in feature_dtypes.items():\n",
    "            if col not in processed.columns:\n",
    "                if 'float' in dtype_name or 'int' in dtype_name:\n",
    "                    processed[col] = 0.0\n",
    "                else:\n",
    "                    processed[col] = 'OUTRA'\n",
    "        processed = processed.loc[:, feature_columns]\n",
    "\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelo Preditivo para Dura\u00e7\u00e3o da Interna\u00e7\u00e3o\n",
    "Mantivemos o Random Forest Regressor porque ele j\u00e1 havia mostrado bom desempenho. A diferen\u00e7a \u00e9 que agora refor\u00e7amos a separa\u00e7\u00e3o entre features/target, descrevemos a engenharia usada no pipeline e preparamos tudo para validar com m\u00faltiplas parti\u00e7\u00f5es do dado.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_modeling = df_prepared.copy()\n",
    "\n",
    "stay_mapping = {\n",
    "    \"0-10\": 5,\n",
    "    \"11-20\": 15,\n",
    "    \"21-30\": 25,\n",
    "    \"31-40\": 35,\n",
    "    \"41-50\": 45,\n",
    "    \"51-60\": 55,\n",
    "    \"61-70\": 65,\n",
    "    \"71-80\": 75,\n",
    "    \"81-90\": 85,\n",
    "    \"91-100\": 95,\n",
    "    \"More than 100 Days\": 110,\n",
    "}\n",
    "\n",
    "stay_mapping_reverse = {v: k for k, v in stay_mapping.items()}\n",
    "\n",
    "df_modeling[\"Stay_numeric\"] = df_modeling[\"Stay\"].map(stay_mapping)\n",
    "df_modeling = df_modeling.dropna(subset=[\"Stay_numeric\"])\n",
    "\n",
    "id_columns = [\"case_id\", \"patientid\"]\n",
    "X = df_modeling.drop(columns=id_columns + [\"Stay\", \"Stay_numeric\"], errors=\"ignore\")\n",
    "y = df_modeling[\"Stay_numeric\"]\n",
    "\n",
    "numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", regressor)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n",
    "\n",
    "joblib.dump(pipeline, \"random_forest_pipeline.pkl\")\n",
    "\n",
    "preparation_artifacts.update(\n",
    "    {\n",
    "        \"feature_columns\": X.columns.tolist(),\n",
    "        \"feature_dtypes\": X.dtypes.astype(str).to_dict(),\n",
    "        \"id_columns\": id_columns,\n",
    "        \"stay_mapping\": stay_mapping,\n",
    "        \"stay_mapping_reverse\": stay_mapping_reverse,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Valida\u00e7\u00e3o cruzada e baseline\n",
    "Para garantir que o resultado n\u00e3o dependa apenas da divis\u00e3o `train_test_split`, aplicamos valida\u00e7\u00e3o cruzada com 5 parti\u00e7\u00f5es. Tamb\u00e9m treinamos um `DummyRegressor` (que sempre prev\u00ea a mediana) para servir como refer\u00eancia m\u00ednima aceit\u00e1vel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rf_cv_scores = -cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "dummy_baseline = DummyRegressor(strategy='median')\n",
    "baseline_scores = -cross_val_score(dummy_baseline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "validation_results = pd.DataFrame(\n",
    "    {\n",
    "        'Modelo': ['RandomForest', 'Baseline (mediana)'],\n",
    "        'MAE m\u00e9dio (dias)': [rf_cv_scores.mean(), baseline_scores.mean()],\n",
    "        'Desvio-padr\u00e3o do MAE': [rf_cv_scores.std(), baseline_scores.std()],\n",
    "    }\n",
    ").round(2)\n",
    "\n",
    "validation_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores da tabela mostram o **erro absoluto m\u00e9dio (MAE)** em dias. Quanto menor, melhor. O desvio-padr\u00e3o indica o quanto o desempenho varia entre as parti\u00e7\u00f5es. Vemos que o Random Forest apresenta erro m\u00e9dio bem menor e mais est\u00e1vel do que o baseline, validando o esfor\u00e7o extra de prepara\u00e7\u00e3o dos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Conclus\u00e3o do modelo preditivo\n",
    "- O `RandomForestRegressor` manteve MAE em torno de poucos dias tanto no hold-out quanto na valida\u00e7\u00e3o cruzada.\n",
    "- O baseline baseado na mediana tem erro bem maior, mostrando que o modelo realmente aprendeu padr\u00f5es relevantes.\n",
    "- Como o alvo \u00e9 ordinal, ainda podemos explorar modelos espec\u00edficos (ex.: classifica\u00e7\u00e3o ordinal) em trabalhos futuros, mas os resultados atuais j\u00e1 s\u00e3o consistentes para apresenta\u00e7\u00e3o.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def map_prediction_to_stay_category(prediction_value):\n",
    "    closest_stay_numeric = min(\n",
    "        preparation_artifacts[\"stay_mapping_reverse\"].keys(),\n",
    "        key=lambda k: abs(k - prediction_value),\n",
    "    )\n",
    "    return preparation_artifacts[\"stay_mapping_reverse\"][closest_stay_numeric]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Simula\u00e7\u00f5es com Perfis de Pacientes\n",
    "Mantivemos os perfis definidos anteriormente e acrescentamos coment\u00e1rios para interpretar as previs\u00f5es. Isso ajuda a comunicar os resultados para pessoas n\u00e3o t\u00e9cnicas.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "patient_profiles_data = [\n",
    "    {\n",
    "        \"Hospital_code\": 8,\n",
    "        \"Hospital_type_code\": \"a\",\n",
    "        \"City_Code_Hospital\": 1,\n",
    "        \"Hospital_region_code\": \"X\",\n",
    "        \"Available Extra Rooms in Hospital\": 4,\n",
    "        \"Department\": \"gynecology\",\n",
    "        \"Ward_Type\": \"P\",\n",
    "        \"Ward_Facility_Code\": \"F\",\n",
    "        \"Bed Grade\": 2.0,\n",
    "        \"City_Code_Patient\": 7.0,\n",
    "        \"Type of Admission\": \"Emergency\",\n",
    "        \"Severity of Illness\": \"Minor\",\n",
    "        \"Visitors with Patient\": 2,\n",
    "        \"Age\": \"0-10\",\n",
    "        \"Admission_Deposit\": 4000.0,\n",
    "        \"Profile_Name\": \"Jovem, Doen\u00e7a Leve\",\n",
    "    },\n",
    "    {\n",
    "        \"Hospital_code\": 26,\n",
    "        \"Hospital_type_code\": \"b\",\n",
    "        \"City_Code_Hospital\": 2,\n",
    "        \"Hospital_region_code\": \"Y\",\n",
    "        \"Available Extra Rooms in Hospital\": 2,\n",
    "        \"Department\": \"surgery\",\n",
    "        \"Ward_Type\": \"S\",\n",
    "        \"Ward_Facility_Code\": \"F\",\n",
    "        \"Bed Grade\": 3.0,\n",
    "        \"City_Code_Patient\": 8.0,\n",
    "        \"Type of Admission\": \"Trauma\",\n",
    "        \"Severity of Illness\": \"Extreme\",\n",
    "        \"Visitors with Patient\": 4,\n",
    "        \"Age\": \"81-90\",\n",
    "        \"Admission_Deposit\": 6000.0,\n",
    "        \"Profile_Name\": \"Idoso, Doen\u00e7a Extrema\",\n",
    "    },\n",
    "    {\n",
    "        \"Hospital_code\": 10,\n",
    "        \"Hospital_type_code\": \"e\",\n",
    "        \"City_Code_Hospital\": 3,\n",
    "        \"Hospital_region_code\": \"Z\",\n",
    "        \"Available Extra Rooms in Hospital\": 3,\n",
    "        \"Department\": \"radiotherapy\",\n",
    "        \"Ward_Type\": \"T\",\n",
    "        \"Ward_Facility_Code\": \"E\",\n",
    "        \"Bed Grade\": 4.0,\n",
    "        \"City_Code_Patient\": 10.0,\n",
    "        \"Type of Admission\": \"Elective\",\n",
    "        \"Severity of Illness\": \"Moderate\",\n",
    "        \"Visitors with Patient\": 3,\n",
    "        \"Age\": \"51-60\",\n",
    "        \"Admission_Deposit\": 5000.0,\n",
    "        \"Profile_Name\": \"Meia-idade, Doen\u00e7a Moderada\",\n",
    "    },\n",
    "    {\n",
    "        \"Hospital_code\": 1,\n",
    "        \"Hospital_type_code\": \"a\",\n",
    "        \"City_Code_Hospital\": 1,\n",
    "        \"Hospital_region_code\": \"X\",\n",
    "        \"Available Extra Rooms in Hospital\": 1,\n",
    "        \"Department\": \"gynecology\",\n",
    "        \"Ward_Type\": \"P\",\n",
    "        \"Ward_Facility_Code\": \"F\",\n",
    "        \"Bed Grade\": 1.0,\n",
    "        \"City_Code_Patient\": 7.0,\n",
    "        \"Type of Admission\": \"Emergency\",\n",
    "        \"Severity of Illness\": \"Extreme\",\n",
    "        \"Visitors with Patient\": 5,\n",
    "        \"Age\": \"0-10\",\n",
    "        \"Admission_Deposit\": 4500.0,\n",
    "        \"Profile_Name\": \"Jovem, Doen\u00e7a Extrema\",\n",
    "    },\n",
    "    {\n",
    "        \"Hospital_code\": 32,\n",
    "        \"Hospital_type_code\": \"f\",\n",
    "        \"City_Code_Hospital\": 6,\n",
    "        \"Hospital_region_code\": \"Y\",\n",
    "        \"Available Extra Rooms in Hospital\": 5,\n",
    "        \"Department\": \"anesthesia\",\n",
    "        \"Ward_Type\": \"Q\",\n",
    "        \"Ward_Facility_Code\": \"D\",\n",
    "        \"Bed Grade\": 3.0,\n",
    "        \"City_Code_Patient\": 1.0,\n",
    "        \"Type of Admission\": \"Emergency\",\n",
    "        \"Severity of Illness\": \"Minor\",\n",
    "        \"Visitors with Patient\": 1,\n",
    "        \"Age\": \"91-100\",\n",
    "        \"Admission_Deposit\": 3500.0,\n",
    "        \"Profile_Name\": \"Idoso, Doen\u00e7a Leve\",\n",
    "    },\n",
    "]\n",
    "\n",
    "patient_profiles_df = pd.DataFrame(patient_profiles_data)\n",
    "profile_metadata = patient_profiles_df[[\"Profile_Name\"]].copy()\n",
    "\n",
    "patient_features = prepare_dataset_for_inference(\n",
    "    patient_profiles_df.drop(columns=[\"Profile_Name\"]),\n",
    "    preparation_artifacts,\n",
    "    drop_target=True,\n",
    ")\n",
    "\n",
    "profile_predictions_numeric = pipeline.predict(patient_features)\n",
    "\n",
    "predicted_stay_categories = [\n",
    "    map_prediction_to_stay_category(value) for value in profile_predictions_numeric\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Profile_Name\": profile_metadata[\"Profile_Name\"],\n",
    "        \"Predicted_Stay_Numeric\": profile_predictions_numeric,\n",
    "        \"Predicted_Stay_Category\": predicted_stay_categories,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Previs\u00f5es para os perfis de pacientes:\")\n",
    "print(results_df.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Previs\u00e3o no Conjunto de Teste do BigQuery\n",
    "Quando estivermos conectados ao BigQuery, esta etapa funciona exatamente como no notebook original. Caso o notebook esteja rodando apenas com os CSVs, `test` vir\u00e1 do arquivo local e poderemos gerar previs\u00f5es para revis\u00e3o interna.\n",
    "\n",
    "> **Nota:** Ainda n\u00e3o escrevemos os resultados de volta para o BigQuery. O c\u00f3digo permanece pronto para isso; basta ajustar quando o time decidir liberar o acesso.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_features = prepare_dataset_for_inference(\n",
    "    test,\n",
    "    preparation_artifacts,\n",
    "    drop_target=True,\n",
    ")\n",
    "\n",
    "test_predictions_numeric = pipeline.predict(test_features)\n",
    "test_predicted_categories = [\n",
    "    map_prediction_to_stay_category(value) for value in test_predictions_numeric\n",
    "]\n",
    "\n",
    "test_results = test.copy()\n",
    "test_results[\"Stay_predicted\"] = test_predictions_numeric\n",
    "test_results[\"Stay_category\"] = test_predicted_categories\n",
    "\n",
    "test_results[[\"case_id\", \"Stay_predicted\", \"Stay_category\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Sugest\u00f5es de Interven\u00e7\u00f5es para Otimizar a Dura\u00e7\u00e3o da Interna\u00e7\u00e3o\n",
    "Com a limpeza mais consistente e a valida\u00e7\u00e3o cruzada, ganhamos confian\u00e7a para propor a\u00e7\u00f5es. As ideias abaixo combinam achados da an\u00e1lise explorat\u00f3ria e os cen\u00e1rios simulados.\n",
    "\n",
    "1. **Revisar fluxos de pacientes com gravidade extrema:** priorizar aloca\u00e7\u00e3o de quartos extras e equipe multidisciplinar para casos cr\u00edticos que chegam em regime de emerg\u00eancia.\n",
    "2. **Monitorar dep\u00f3sitos de admiss\u00e3o elevados:** esses casos tendem a indicar procedimentos caros e longos; sugerimos acompanhamento financeiro antecipado.\n",
    "3. **Investigar hospitais com baixa disponibilidade de quartos extras:** podem se beneficiar de ajustes de agenda ou transfer\u00eancias planejadas.\n",
    "4. **Pr\u00f3ximos passos t\u00e9cnicos:** testar modelos ordinais, avaliar import\u00e2ncia de vari\u00e1veis e preparar scripts para atualizar o pipeline quando o BigQuery receber dados novos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}